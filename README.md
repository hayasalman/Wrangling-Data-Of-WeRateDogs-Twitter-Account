## Wrangling Data Of **WeRateDogs** Twitter Account

![weratedogs__](https://github.com/hayasalman/Wrangling-Dataset/assets/71796909/cee95e5b-7fda-4bf8-a0d7-73f41284147b)

## Overview 

Real-world data rarely comes clean. Using Python and its libraries, we will gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. And the dataset that we will be wrangling is the tweet archive of Twitter user. 
WeRateDogs **(@dog_rates)** is a Twitter account that rates people's dogs with a humorous comment about the dog. The account was started in 2015 by college student Matt Nelson, and has received international media attention both for its popularity.

## About The Dataset

- This dataset contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017.
  
- Data sources stored in different formats and can be accessed through this links :

  - [Image Predictions TSV File](https://github.com/hayasalman/Wrangling-Dataset/blob/main/Datasets/image-predictions.tsv)
 
  - [Tweets JSON File](https://raw.githubusercontent.com/hayasalman/Wrangling-Dataset/main/Datasets/tweet_json.txt)
 
  - [Twitter Archive Master CSV File](https://github.com/hayasalman/Wrangling-Dataset/blob/main/Datasets/twitter_archive_master.csv)
 
  - [Twitter Archive Enhanced CSV File](https://github.com/hayasalman/Wrangling-Dataset/blob/main/Datasets/twitter-archive-enhanced.csv)
 
  ## Coding

  -  Python Integrated Development Environment (IDE) : Jupyter Notebooks.

   **Packeges used**
   
  * **pandas - numby** : used for data manipulation.
  * **matplotlib** : used for data visualtion.
  * **requests** : used for HTTP requests.
  * **os** : used for interacting with the operating system.
  * **tweepy** : used for accessing the Twitter API.
 
  ## Approaches & Methodologies

  - **Gathering data** : the first step of data wrangling is collect data from different sources.
 
  - **Assessing data** : assessing the data both visaully & programmatically to discover any quality\tidness issues. The four main data quality 
      dimensions are: Accuracy, Validity, Completeness &Consistency , while the structural issues related to tidness issues.
 
  - **Cleaning data** : after the data assessment we will start to clean it from the quality\tidness issues that we found before.
 
  - **Storing data** : we need to store the cleaned dataset into new csv file called twitter archive master before we analyze it.
 
  - **Analyzing , and visualizing data** : by perform exploraty data analysis.
 
  - **Communicate findings** : through communicates all the insights and displays the visualization(s) produced from your wrangled data.
 
  ## Business Insights

  ## Refernces

  

  

